{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140649ed-88a5-4682-9053-0d5763a984f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu: True\n",
      "The model has 315,802,126 trainable parameters\n",
      "Epoch: 1 | Time: 1m 19s\n",
      "\tTrain Loss: 0.623 | Train PPL:   1.864\n",
      "\tVal Loss: 0.398 |  Val PPL:   1.489\n",
      "Epoch: 2 | Time: 1m 18s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
      "\tVal Loss: 0.376 |  Val PPL:   1.456\n",
      "Epoch: 3 | Time: 1m 18s\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\tVal Loss: 0.375 |  Val PPL:   1.455\n",
      "Epoch: 4 | Time: 1m 18s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.430\n",
      "\tVal Loss: 0.376 |  Val PPL:   1.456\n",
      "Epoch: 5 | Time: 1m 18s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\tVal Loss: 0.375 |  Val PPL:   1.455\n",
      "Epoch: 6 | Time: 1m 18s\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.453\n",
      "Epoch: 7 | Time: 1m 18s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.454\n",
      "Epoch: 8 | Time: 1m 18s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.454\n",
      "Epoch: 9 | Time: 1m 18s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.424\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.453\n",
      "Epoch: 10 | Time: 1m 18s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.424\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.453\n",
      "Epoch: 11 | Time: 1m 18s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.454\n",
      "Epoch: 12 | Time: 1m 18s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\tVal Loss: 0.373 |  Val PPL:   1.452\n",
      "Epoch: 13 | Time: 1m 18s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\tVal Loss: 0.375 |  Val PPL:   1.455\n",
      "Epoch: 14 | Time: 1m 18s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.454\n",
      "Epoch: 15 | Time: 1m 18s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.454\n",
      "Epoch: 16 | Time: 1m 17s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\tVal Loss: 0.373 |  Val PPL:   1.452\n",
      "Epoch: 17 | Time: 1m 18s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.453\n",
      "Epoch: 18 | Time: 1m 17s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\tVal Loss: 0.373 |  Val PPL:   1.452\n",
      "Epoch: 19 | Time: 1m 18s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\tVal Loss: 0.373 |  Val PPL:   1.452\n",
      "Epoch: 20 | Time: 1m 18s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\tVal Loss: 0.373 |  Val PPL:   1.452\n",
      "Epoch: 21 | Time: 1m 18s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\tVal Loss: 0.372 |  Val PPL:   1.451\n",
      "Epoch: 22 | Time: 1m 18s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.453\n",
      "Epoch: 23 | Time: 1m 18s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.420\n",
      "\tVal Loss: 0.371 |  Val PPL:   1.449\n",
      "Epoch: 24 | Time: 1m 18s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.420\n",
      "\tVal Loss: 0.371 |  Val PPL:   1.450\n",
      "Epoch: 25 | Time: 1m 18s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\tVal Loss: 0.372 |  Val PPL:   1.450\n",
      "Epoch: 26 | Time: 1m 18s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\tVal Loss: 0.372 |  Val PPL:   1.450\n",
      "Epoch: 27 | Time: 1m 18s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\tVal Loss: 0.373 |  Val PPL:   1.451\n",
      "Epoch: 28 | Time: 1m 18s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\tVal Loss: 0.375 |  Val PPL:   1.455\n",
      "Epoch: 29 | Time: 1m 18s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.454\n",
      "Epoch: 30 | Time: 1m 18s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.417\n",
      "\tVal Loss: 0.375 |  Val PPL:   1.455\n",
      "Epoch: 31 | Time: 1m 18s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\tVal Loss: 0.375 |  Val PPL:   1.455\n",
      "Epoch: 32 | Time: 1m 18s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\tVal Loss: 0.377 |  Val PPL:   1.458\n",
      "Epoch: 33 | Time: 1m 18s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\tVal Loss: 0.378 |  Val PPL:   1.460\n",
      "Epoch: 34 | Time: 1m 18s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\tVal Loss: 0.374 |  Val PPL:   1.453\n",
      "Epoch: 35 | Time: 1m 18s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.414\n",
      "\tVal Loss: 0.380 |  Val PPL:   1.463\n",
      "Epoch: 36 | Time: 1m 18s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\tVal Loss: 0.378 |  Val PPL:   1.459\n",
      "Epoch: 37 | Time: 1m 18s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\tVal Loss: 0.379 |  Val PPL:   1.461\n",
      "Epoch: 38 | Time: 1m 18s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\tVal Loss: 0.376 |  Val PPL:   1.457\n",
      "Epoch: 39 | Time: 1m 18s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\tVal Loss: 0.378 |  Val PPL:   1.459\n",
      "Epoch: 40 | Time: 1m 18s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\tVal Loss: 0.377 |  Val PPL:   1.458\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "\n",
    "from data import *\n",
    "# data用于获取数据集，主要用DataLoader获取数据集\n",
    "# 同时data中导入了conf，因此参数设置也在data中，如device\n",
    "# 包括：\n",
    "# train\n",
    "# valid\n",
    "# test\n",
    "\n",
    "from model.LMVCAT import LMVCATModel\n",
    "from loss.Loss import Loss\n",
    "# __init__.py可以把一个文件夹变成一个包，组织清晰便于维护\n",
    "# 模型放在models中\n",
    "\n",
    "from tokens import get_tokens\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# count_parameters可以计算模型参数量\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "# kaiming_uniform初始化参数\n",
    "\n",
    "print(\"gpu:\", torch.cuda.is_available())\n",
    "cls_tokens = get_tokens()\n",
    "\n",
    "model = LMVCATModel(n_view=n_view,\n",
    "                    d_vec=d_vec,\n",
    "                    mlp_out=mlp_out,\n",
    "                    d_model=d_model,\n",
    "                    mlp_hidden=mlp_hidden,\n",
    "                    drop_prob=drop_prob,\n",
    "                    vf_hidden=vf_hidden,\n",
    "                    vf_head=vf_head,\n",
    "                    vf_layers=vf_layers,\n",
    "                    awf_gamma=awf_gamma,\n",
    "                    cf_hidden=cf_hidden,\n",
    "                    cf_head=cf_head,\n",
    "                    cf_layers=cf_layers,\n",
    "                    cls_tokens=cls_tokens,\n",
    "                    s_mask=s_mask,\n",
    "                    l_mask=l_mask,\n",
    "                    n_cls=n_cls)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model.to(device)\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "optimizer = Adam(params=model.parameters(),\n",
    "                 lr=init_lr,\n",
    "                 weight_decay=weight_decay,\n",
    "                 eps=adam_eps)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)\n",
    "\n",
    "criterion = Loss(d_model=mlp_out,\n",
    "                 n_cls=n_cls,\n",
    "                 alpha=alpha,\n",
    "                 beta=beta,\n",
    "                 s_mask=s_mask,\n",
    "                 l_mask=l_mask)\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip=None):\n",
    "    \"\"\"\n",
    "    train函数是一个epoch内的，也仅返回一个epoch的loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch[0].cuda(non_blocking=True)  #  [batch_size, n_view, d_vec]\n",
    "        trg = batch[1].cuda(non_blocking=True)  #  [batch_size, n_cls]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        v, z, p = model(src)\n",
    "\n",
    "        loss = criterion(v, z, p, trg)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # 剪裁梯度范围\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        # print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    计算验证集上误差\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0].cuda(non_blocking=True)\n",
    "            trg = batch[1].cuda(non_blocking=True)\n",
    "            v, z, p = model(src)\n",
    "\n",
    "            loss = criterion(v, z, p, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def run(total_epoch, best_loss):\n",
    "    \"\"\"\n",
    "    除了train跟evaluation外，一般用run将二者组合起来\n",
    "    \"\"\"\n",
    "    train_iter, valid_iter = get_data_loader()\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    for step in range(total_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss = train(model, train_iter, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, valid_iter, criterion)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # 在warmup前learning rate不变\n",
    "        if step > warmup:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        # best_loss计算最佳验证集loss，并且在loss最佳时保存模型\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'saved/model-{0}.pt'.format(valid_loss))\n",
    "\n",
    "        f = open('result/train_loss.txt', 'w')\n",
    "        f.write(str(train_losses))\n",
    "        f.close()\n",
    "\n",
    "        f = open('result/test_loss.txt', 'w')\n",
    "        f.write(str(test_losses))\n",
    "        f.close()\n",
    "\n",
    "        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run(total_epoch=epoch, best_loss=inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460e03f-75bb-4411-8c27-4c48b9eabff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
